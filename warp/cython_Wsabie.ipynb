{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "from random import randint\n",
    "from data_load import *\n",
    "in_dir= \"/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN\"\n",
    "a=MentionData('/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/train_x_new.txt',\n",
    "              \"/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/train_y.txt\",\n",
    "             in_dir+\"/feature.txt\",in_dir+\"/type.txt\")\n",
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "\n",
    "cimport numpy as np\n",
    "from random import randint\n",
    "import sys\n",
    "import cython\n",
    "\n",
    "import math\n",
    "from libc.stdlib cimport malloc, free\n",
    "\n",
    "from libc.math cimport exp\n",
    "from libc.math cimport log\n",
    "from libc.math cimport sqrt\n",
    "from libc.string cimport memset\n",
    "import random\n",
    "# scipy <= 0.15\n",
    "\n",
    "import scipy.linalg.blas as fblas\n",
    "ctypedef np.float32_t REAL_t\n",
    "cdef int ONE = 1\n",
    "\n",
    "\n",
    "REAL = np.float32\n",
    "cdef extern from \"/Users/mayk/working/figer/baseline/PLE/Model/warp/voidptr.h\":\n",
    "    void* PyCObject_AsVoidPtr(object obj)\n",
    "DEF MAX_SENTENCE_LEN = 10000\n",
    "ctypedef void (*scopy_ptr) (const int *N, const float *X, const int *incX, float *Y, const int *incY) nogil\n",
    "ctypedef void (*saxpy_ptr) (const int *N, const float *alpha, const float *X, const int *incX, float *Y, const int *incY) nogil\n",
    "ctypedef float (*sdot_ptr) (const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil\n",
    "ctypedef double (*dsdot_ptr) (const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil\n",
    "ctypedef double (*snrm2_ptr) (const int *N, const float *X, const int *incX) nogil\n",
    "ctypedef void (*sscal_ptr) (const int *N, const float *alpha, const float *X, const int *incX) nogil\n",
    "\n",
    "\n",
    "cdef scopy_ptr scopy = <scopy_ptr>PyCObject_AsVoidPtr(fblas.scopy._cpointer)  # y = x\n",
    "cdef saxpy_ptr saxpy=<saxpy_ptr>PyCObject_AsVoidPtr(fblas.saxpy._cpointer)  # y += alpha * x\n",
    "cdef sdot_ptr sdot=<sdot_ptr>PyCObject_AsVoidPtr(fblas.sdot._cpointer)  # float = dot(x, y)\n",
    "cdef dsdot_ptr dsdot=<dsdot_ptr>PyCObject_AsVoidPtr(fblas.sdot._cpointer)  # double = dot(x, y)\n",
    "cdef snrm2_ptr snrm2=<snrm2_ptr>PyCObject_AsVoidPtr(fblas.snrm2._cpointer)  # sqrt(x^2)\n",
    "cdef sscal_ptr sscal=<sscal_ptr>PyCObject_AsVoidPtr(fblas.sscal._cpointer) # x = alpha * x\n",
    "DEF EXP_TABLE_SIZE = 10000\n",
    "DEF MAX_EXP = 50\n",
    "\n",
    "cdef REAL_t[EXP_TABLE_SIZE] EXP_TABLE\n",
    "cdef REAL_t[EXP_TABLE_SIZE] LOG_TABLE\n",
    "\n",
    "cdef REAL_t ONEF = <REAL_t>1.0\n",
    "\n",
    "# for when fblas.sdot returns a double\n",
    "cdef REAL_t our_dot_double(const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil:\n",
    "    return <REAL_t>dsdot(N, X, incX, Y, incY)\n",
    "\n",
    "# for when fblas.sdot returns a float\n",
    "cdef REAL_t our_dot_float(const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil:\n",
    "    return <REAL_t>sdot(N, X, incX, Y, incY)\n",
    "\n",
    "# for when no blas available\n",
    "cdef REAL_t our_dot_noblas(const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil:\n",
    "    # not a true full dot()-implementation: just enough for our cases\n",
    "    cdef int i\n",
    "    cdef REAL_t a\n",
    "    a = <REAL_t>0.0\n",
    "    for i from 0 <= i < 50 by 1:\n",
    "        a += X[i] * Y[i]\n",
    "    return a\n",
    "\n",
    "# for when no blas available\n",
    "cdef void our_saxpy_noblas(const int *N, const float *alpha, const float *X, const int *incX, float *Y, const int *incY) nogil:\n",
    "    cdef int i\n",
    "    for i from 0 <= i < N[0] by 1:\n",
    "        Y[i * (incY[0])] = (alpha[0]) * X[i * (incX[0])] + Y[i * (incY[0])]\n",
    "cdef REAL_t cvdot(vec1,vec2,size):\n",
    "    cdef int csize = size\n",
    "    f= dsdot(&csize,<REAL_t *>(np.PyArray_DATA(vec1)),&ONE,<REAL_t *>(np.PyArray_DATA(vec2)),&ONE)\n",
    "    return f\n",
    "def csaxpy(vec1,vec2,alpha,size):\n",
    "    cdef int csize = size\n",
    "    cdef float calpha = alpha\n",
    "    f= our_saxpy_noblas(&csize,&calpha,<REAL_t *>(np.PyArray_DATA(vec1)),&ONE,<REAL_t *>(np.PyArray_DATA(vec2)),&ONE)\n",
    "    return f\n",
    "cdef REAL_t crank(int k):\n",
    "    cdef REAL_t loss = 0.\n",
    "    cdef int i = 1\n",
    "    for i in xrange(1,k+1):\n",
    "        loss += ONEF/i\n",
    "    return loss\n",
    "cdef REAL_t vsum(REAL_t *vec,int *size):\n",
    "    cdef int i\n",
    "    cdef REAL_t product\n",
    "    product = <REAL_t>0.0\n",
    "    for i from 0 <= i < size[0] by 1:\n",
    "        product += vec[i] * vec[i]\n",
    "    return np.sqrt(product)\n",
    "def cnorm(vec):\n",
    "    cdef int size\n",
    "    size  = len(vec)\n",
    "    return vsum(<REAL_t *>(np.PyArray_DATA(vec)),&size)\n",
    "def init():\n",
    "    for i in range(EXP_TABLE_SIZE):\n",
    "        EXP_TABLE[i] = <REAL_t>exp((i / <REAL_t>EXP_TABLE_SIZE * 2 - 1) * MAX_EXP)\n",
    "        EXP_TABLE[i] = <REAL_t>(EXP_TABLE[i] / (EXP_TABLE[i] + 1))\n",
    "#init()\n",
    "\n",
    "\n",
    "def ctrain(A,B,insts,size,lr,gradient,max_it =10):\n",
    "    cdef float error\n",
    "\n",
    "    for it in xrange(1,max_it+1):\n",
    "        error = 0.\n",
    "        for i,inst in enumerate(insts):\n",
    "            error+=gradient(A,B,inst,size,lr=lr)\n",
    "        \n",
    "            if i % 1000 ==0:\n",
    "                sys.stdout.write(\"\\rIteration %d \" % (it)+ \"trained {0:.0f}%\".format(float(i)*100/len(insts))+\" Loss:{0:.2f}\".format(error))\n",
    "                sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\n\")\n",
    "    return error\n",
    "\n",
    "cdef void divide(REAL_t *vec, const float *alpha, const int *size):\n",
    "    cdef int i\n",
    "    cdef REAL_t product\n",
    "    for i from 0 <= i < size[0] by 1:\n",
    "        vec[i] = vec[i]/alpha[0]\n",
    "def cdivide(vec,alpha):\n",
    "    cdef int size\n",
    "    size  = len(vec)\n",
    "    cdef float r = alpha\n",
    "    divide(<REAL_t *>(np.PyArray_DATA(vec)),&r,&size)\n",
    "\n",
    "    \n",
    "def max_margin_gradient(A,B,inst,size,lr=0.01):\n",
    "    #print B\n",
    "    #print B[0]-B[9]\n",
    "\n",
    "    dA = np.zeros(size,dtype=REAL)\n",
    "    dB = np.zeros([len(inst.labels),size],dtype=REAL)\n",
    "    random.seed(1)\n",
    "    x = np.sum(A[inst.features],axis=0)\n",
    "    cdef REAL_t error = 0\n",
    "    cdef REAL_t s1,s2\n",
    "    cdef clr = lr\n",
    "    cdef int N\n",
    "    cdef int n_sample\n",
    "    cdef int neg_num = len(inst.negative_labels)\n",
    "    cdef REAL_t norm\n",
    "    for l in inst.sparse_labels:\n",
    "        s1= cvdot(x,B[l],50)\n",
    "        N=1\n",
    "        n_sample  = -1\n",
    "        for k in xrange(neg_num):\n",
    "            nl = inst.negative_labels[randint(0,neg_num-1)]\n",
    "            s2 = cvdot(x,B[nl],50)\n",
    "            if s1 - s2<1:\n",
    "                n_sample = nl\n",
    "                N = k+1\n",
    "                break\n",
    "        if n_sample!=-1:\n",
    "            error += 1 + s2-s1\n",
    "            csaxpy(B[l]-B[n_sample],dA,1.0,50)\n",
    "            csaxpy(x,dB[l],1.0,50)\n",
    "            csaxpy(x,dB[n_sample],-1.0,50)\n",
    "\n",
    "    for f in inst.features:\n",
    "        csaxpy(dA,A[f],clr,50)\n",
    "        norm = cnorm(A[f])\n",
    "        if norm >1:\n",
    "            cdivide(A[f],norm)\n",
    "\n",
    "    for i in xrange(len(B)):\n",
    "        csaxpy(dB[i],B[i],clr,50)\n",
    "        #B[i] += lr*dB[i]\n",
    "        norm =  cnorm(B[i])\n",
    "        if norm >1:\n",
    "            cdivide(B[i],norm)\n",
    "            #B[i] /=norm\n",
    "    return error\n",
    "\n",
    "def max_max_margin_gradient(A,B,inst,size,lr=0.01):\n",
    "    #print B\n",
    "    #print B[0]-B[9]\n",
    "\n",
    "    dA = np.zeros(size,dtype=REAL)\n",
    "    dB = np.zeros([len(inst.labels),size],dtype=REAL)\n",
    "    random.seed(1)\n",
    "    x = np.sum(A[inst.features],axis=0)\n",
    "    cdef REAL_t error = 0\n",
    "    cdef REAL_t s1,s2\n",
    "    cdef clr = lr\n",
    "    cdef int N\n",
    "    cdef int neg_num = len(inst.negative_labels)\n",
    "    cdef REAL_t norm\n",
    "    cdef int max_l,max_nl\n",
    "    cdef REAL_t max_s1= float('-inf'),max_s2=float('-inf')\n",
    "    for l in inst.sparse_labels:\n",
    "        s1= cvdot(x,B[l],50)\n",
    "        if s1>=max_s1:\n",
    "            max_l = l\n",
    "            max_s1 = s1\n",
    "    for nl in inst.negative_labels:\n",
    "        s2= cvdot(x,B[nl],50)\n",
    "        if s2>=max_s2:\n",
    "            max_nl = nl\n",
    "            max_s2 = s2\n",
    "    if max_s1-max_s2<1:\n",
    "        error += 1 + max_s2-max_s1\n",
    "        csaxpy(B[max_l]-B[max_nl],dA,1.0,50)\n",
    "        csaxpy(x,dB[max_l],1.0,50)\n",
    "        csaxpy(x,dB[max_nl],-1.0,50)\n",
    "\n",
    "    for f in inst.features:\n",
    "        csaxpy(dA,A[f],clr,50)\n",
    "        norm = cnorm(A[f])\n",
    "        if norm >1:\n",
    "            cdivide(A[f],norm)\n",
    "\n",
    "    for i in xrange(len(B)):\n",
    "        csaxpy(dB[i],B[i],clr,50)\n",
    "        #B[i] += lr*dB[i]\n",
    "        norm =  cnorm(B[i])\n",
    "        if norm >1:\n",
    "            cdivide(B[i],norm)\n",
    "            #B[i] /=norm\n",
    "    return error\n",
    "def softmax_gradient(A,B,inst,size,lr=0.01):\n",
    "    #print B\n",
    "    #print B[0]-B[9]\n",
    "\n",
    "    dA = np.zeros(size,dtype=REAL)\n",
    "    random.seed(1)\n",
    "    x = np.mean(A[inst.features],axis=0)\n",
    "    cdef REAL_t error = 0\n",
    "    cdef REAL_t s1,s2,logs1,logs2,g\n",
    "    cdef clr = lr\n",
    "    cdef int neg_num = len(inst.negative_labels)\n",
    "    cdef REAL_t norm\n",
    "    cdef REAL_t pos = ONEF\n",
    "    cdef int csize =size\n",
    "    for l in inst.sparse_labels:\n",
    "        logs1= cvdot(x,B[l],50)\n",
    "        if  logs1 <= -MAX_EXP or  logs1 >= MAX_EXP:\n",
    "            continue\n",
    "        s1 = EXP_TABLE[<int>((logs1 + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]\n",
    "        error -=logs1\n",
    "        g = (pos - s1) * clr\n",
    "        csaxpy(B[l],dA,g,50)\n",
    "        saxpy(&csize, &g, <REAL_t *>(np.PyArray_DATA(x)), &ONE, <REAL_t *>(np.PyArray_DATA(B[l])), &ONE)\n",
    "        for k in xrange(neg_num):\n",
    "            nl = inst.negative_labels[randint(0,neg_num-1)]\n",
    "            logs2 = cvdot(x,B[nl],50)\n",
    "            if  logs2 <= -MAX_EXP or  logs2 >= MAX_EXP:\n",
    "                continue\n",
    "            s2 = EXP_TABLE[<int>((logs2 + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]\n",
    "            if s2 > s1:\n",
    "                g =  (-s2) * clr\n",
    "                csaxpy(B[nl],dA,g,50)\n",
    "                saxpy(&csize, &g, <REAL_t *>(np.PyArray_DATA(x)), &ONE, <REAL_t *>(np.PyArray_DATA(B[nl])), &ONE)\n",
    "                error += logs2\n",
    "    \n",
    "\n",
    "    for f in inst.features:\n",
    "        csaxpy(dA,A[f],clr,50)\n",
    "        norm = cnorm(A[f])\n",
    "        #if norm >1:\n",
    "         #   cdivide(A[f],norm)\n",
    "\n",
    "    for i in xrange(len(B)):\n",
    "        norm =  cnorm(B[i])\n",
    "       # if norm >1:\n",
    "        #    cdivide(B[i],norm)\n",
    "            #B[i] /=norm\n",
    "    return error\n",
    "\n",
    "def warp_gradient(A,B,inst,size,lr=0.01):\n",
    "    #print B\n",
    "    #print B[0]-B[9]\n",
    "    dA = np.zeros(size,dtype=REAL)\n",
    "    dB = np.zeros([len(inst.labels),size],dtype=REAL)\n",
    "    random.seed(1)\n",
    "    x = np.sum(A[inst.features],axis=0)\n",
    "    cdef REAL_t error = 0.\n",
    "\n",
    "    cdef REAL_t clr = lr\n",
    "    cdef int N,n_sample \n",
    "    cdef int neg_num = len(inst.negative_labels)\n",
    "    cdef REAL_t norm\n",
    "    cdef int cSize = size\n",
    "    cdef REAL_t floats\n",
    "    \n",
    "   \n",
    "    for l in inst.sparse_labels:\n",
    "        s1= cvdot(x,B[l],50)\n",
    "        N=1\n",
    "        n_sample  = -1\n",
    "        for k in xrange(neg_num):\n",
    "            nl = inst.negative_labels[randint(0,neg_num-1)]\n",
    "            s2 = cvdot(x,B[nl],50)\n",
    "            \n",
    "            if s1 - s2<1:\n",
    "                n_sample = nl\n",
    "                N = k+1\n",
    "                break\n",
    "        if n_sample!=-1:\n",
    "\n",
    "            L = crank(len(inst.negative_labels)/N)\n",
    "            negL = -L\n",
    "            error += (1+s2-s1)*L\n",
    "\n",
    "            csaxpy(B[l]-B[n_sample],dA,L,50)\n",
    "            \n",
    "            csaxpy(x,dB[l],L,50)\n",
    "            csaxpy(x,dB[n_sample],-L,50)\n",
    "            #print dB[l][0]\n",
    "    for f in inst.features:\n",
    "        csaxpy(dA,A[f],clr,50)\n",
    "        norm = cnorm(A[f])\n",
    "        if norm >1:\n",
    "            cdivide(A[f],norm)\n",
    "\n",
    "    for i in xrange(len(B)):\n",
    "        csaxpy(dB[i],B[i],clr,50)\n",
    "\n",
    "        #B[i] += lr*dB[i]\n",
    "        norm =  cnorm(B[i])\n",
    "        if norm >1:\n",
    "            cdivide(B[i],norm)\n",
    "            #B[i] /=norm\n",
    "    return error\n",
    "def save_to_text(matrix,output):\n",
    "    shape = matrix.shape\n",
    "    with open(output,'wb') as out:\n",
    "        out.write(\"%d %d\\n\" % (shape))\n",
    "        for row in matrix:\n",
    "            x = \" \".join(map(lambda x:\"{0:.5}\".format(x),row))\n",
    "            out.write(x+\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 trained 100% Loss:435254.88\n",
      "Iteration 2 trained 100% Loss:71137.88\n"
     ]
    }
   ],
   "source": [
    "size=50\n",
    "np.random.seed(1)\n",
    "A= np.random.rand(len(a.feature2id),size).astype(np.float32)\n",
    "# for i in xrange(len(A)):\n",
    "#     norm =  cnorm(A[i])\n",
    "#     if norm >1:\n",
    "#         cdivide(A[i],norm)\n",
    "B= np.random.rand(len(a.label2id),size).astype(np.float32)\n",
    "# for i in xrange(len(B)):\n",
    "#     norm =  cnorm(B[i])\n",
    "#     if norm >1:\n",
    "#         cdivide(B[i],norm)\n",
    "ctrain(A,B,a.data,50,0.01,warp_gradient,max_it=2)\n",
    "\n",
    "#save_to_text(A,'/Users/mayk/working/figer/baseline/PLE/Results/warp_py_A.txt')\n",
    "#save_to_text(B,'/Users/mayk/working/figer/baseline/PLE/Results/warp_py_B.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 trained 80% Loss:27027.41\n",
      "Iteration 2 trained 80% Loss:7855.78\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cdef np.uint32_t *d =<np.uint32_t *>np.PyArray_DATA(np.asarray([[1,2],[1,3]],dtype=np.uint32))\n",
    "print d[0],d[1],d[2],d[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "try:\n",
    "    from queue import Queue, Empty\n",
    "except ImportError:\n",
    "    from Queue import Queue, Empty\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "size=50\n",
    "np.random.seed(1)\n",
    "A= np.random.rand(len(a.feature2id),size).astype(np.float32)\n",
    "# for i in xrange(len(A)):\n",
    "#     norm =  cnorm(A[i])\n",
    "#     if norm >1:\n",
    "#         cdivide(A[i],norm)\n",
    "B= np.random.rand(len(a.label2id),size).astype(np.float32)\n",
    "insts = a.data\n",
    "def train(A,B,insts,it):\n",
    "    queue_factor = 3\n",
    "    works = 8\n",
    "    batch = 2000\n",
    "    def job_producer():\n",
    "        \"\"\"Fill jobs queue using the input `sentences` iterator.\"\"\"\n",
    "        job_batch, batch_size = [], 0\n",
    "        pushed_words, pushed_examples = 0, 0\n",
    "        job_no = 0\n",
    "\n",
    "        for inst_idx, inst in enumerate(insts):\n",
    "            # can we fit this sentence into the existing job batch?\n",
    "            if batch_size + 1 <= batch:\n",
    "                # yes => add it to the current job\n",
    "                job_batch.append(inst)\n",
    "                batch_size += 1\n",
    "            else:\n",
    "                # no => submit the existing job\n",
    "                logger.debug(\n",
    "                    \"queueing job #%i (%i words, %i sentences)\",\n",
    "                    job_no, batch_size, len(job_batch))\n",
    "                job_no += 1\n",
    "                job_queue.put((job_batch))\n",
    "                # add the sentence that didn't fit as the first item of a new job\n",
    "                job_batch, batch_size = [inst], 1\n",
    "        if job_batch:\n",
    "            logger.debug(\n",
    "                \"queueing job #%i (%i insts, %i insts)\",\n",
    "                job_no, batch_size, len(job_batch))\n",
    "            job_no += 1\n",
    "            job_queue.put((job_batch))\n",
    "        for _ in xrange(works):\n",
    "            job_queue.put(None)\n",
    "    def worker_loop():\n",
    "        jobs_processed = 0\n",
    "        while True:\n",
    "            job = job_queue.get()\n",
    "            if job is None:\n",
    "                progress_queue.put(None)\n",
    "                break  # no more jobs => quit this worker\n",
    "            insts = job\n",
    "            tally = ctrain(A,B,insts,50,0.01,warp_gradient,max_it=1)\n",
    "            progress_queue.put((len(insts), tally))  # report back progress\n",
    "            jobs_processed += 1\n",
    "    job_queue = Queue(maxsize=queue_factor *works )\n",
    "    progress_queue = Queue(maxsize=(queue_factor + 1) * works)\n",
    "    workers = [threading.Thread(target=worker_loop) for _ in xrange(works)]\n",
    "    unfinished_worker_count = len(workers)\n",
    "    workers.append(threading.Thread(target=job_producer))\n",
    "    for thread in workers:\n",
    "        thread.daemon = True  # make interrupting the process with ctrl+c easier\n",
    "        thread.start()\n",
    "    err =0\n",
    "    job_tally=0\n",
    "    while unfinished_worker_count > 0:\n",
    "        report = progress_queue.get()  # blocks if workers too slow\n",
    "        if report != None:\n",
    "            err += report[1]\n",
    "            sys.stdout.write(\"\\rIteration %d \" % (it)+ \" Loss:{0:.2f}\".format(err))\n",
    "        if report is None:  # a thread reporting that it finished\n",
    "            unfinished_worker_count -= 1\n",
    "            logger.info(\"worker thread finished; awaiting finish of %i more threads\", unfinished_worker_count)\n",
    "            continue\n",
    "        job_tally += 1\n",
    "        sys.stdout.flush()\n",
    "    sys.stdout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0  Loss:479154.99\n",
      "Iteration 1  Loss:83630.64\n",
      "Iteration 2  Loss:59465.52\n",
      "Iteration 3  Loss:47082.57\n",
      "Iteration 4  Loss:40557.20\n",
      "Iteration 5  Loss:35351.63\n",
      "Iteration 6  Loss:32013.54\n",
      "Iteration 7  Loss:28301.94\n",
      "Iteration 8  Loss:25872.53\n",
      "Iteration 9  Loss:23327.80\n",
      "CPU times: user 3min 51s, sys: 38.7 s, total: 4min 30s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%time for i in range(10):train(A,B,insts,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getLabelLinks(fn):\n",
    "    return processLines(fn,lambda x,y:addFeature(x,y.rstrip().split('\\t')),list())\n",
    "tuples = getLabelLinks('/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/type_type_kb.txt')\n",
    "tuples = [ (int(t[0]),int(t[1]),float(t[2]))for t in tuples]\n",
    "size= 50\n",
    "def corrReg(B,tup,weight,size):\n",
    "    diff = B[tup[0]] - B[tup[1]]\n",
    "    B[tup[0]] -= weight*tup[2]*diff\n",
    "    B[tup[1]] += weight*tup[2]*diff\n",
    "    norm = np.linalg.norm(diff)\n",
    "    return norm * norm * weight*tup[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 trained 100% Loss:435254.88\n",
      "reg error: 1.45011725664\n",
      "Iteration 1 trained 100% Loss:71077.79\n",
      "reg error: 1.25802981709\n",
      "Iteration 1 trained 100% Loss:47323.54\n",
      "reg error: 1.23192860674\n",
      "Iteration 1 trained 100% Loss:35948.48\n",
      "reg error: 1.27711761356\n",
      "Iteration 1 trained 100% Loss:29882.83\n",
      "reg error: 1.3362147477\n",
      "Iteration 1 trained 100% Loss:25117.31\n",
      "reg error: 1.28632881707\n",
      "Iteration 1 trained 100% Loss:22028.33\n",
      "reg error: 1.20925369402\n",
      "Iteration 1 trained 100% Loss:19977.69\n",
      "reg error: 1.17426922764\n",
      "Iteration 1 trained 100% Loss:17786.69\n",
      "reg error: 1.20700317466\n",
      "Iteration 1 trained 100% Loss:16225.64\n",
      "reg error: 1.26068827138\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "A= np.random.rand(len(a.feature2id),size).astype(np.float32)\n",
    "B= np.random.rand(len(a.label2id),size).astype(np.float32)\n",
    "reg_err =0.\n",
    "for i in range(10): \n",
    "    reg_err =0.\n",
    "    ctrain(A,B,a.data,50,0.01,warp_gradient,max_it=1)\n",
    "    for tup in tuples:\n",
    "        reg_err += corrReg(B,tup,0.2,50)\n",
    "    print \"reg error:\",reg_err\n",
    "save_to_text(A,'/Users/mayk/working/figer/baseline/PLE/Results/warp_py_A.txt')\n",
    "save_to_text(B,'/Users/mayk/working/figer/baseline/PLE/Results/warp_py_B.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
