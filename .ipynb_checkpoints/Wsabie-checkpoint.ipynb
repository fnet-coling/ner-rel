{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "from random import randint\n",
    "def lazy_readlines(fn):\n",
    "    with open(fn,'r') as f:\n",
    "        for ln in f:\n",
    "            yield ln\n",
    "def processLines(fn,reduce_fn,init):\n",
    "    return reduce(reduce_fn,lazy_readlines(fn),init)\n",
    "def count_lines(fn):\n",
    "    return processLines(fn,lambda x,y:x+1,0)\n",
    "def addFeature(x,y):\n",
    "    x.append(y)\n",
    "    return x\n",
    "def getAllFeatures(fn):\n",
    "    return processLines(fn,lambda x,y:addFeature(x,[y.rstrip().split()[0],int(y.rstrip().split()[1])]),list())\n",
    "def getAllFeaturesRev(fn):\n",
    "    return processLines(fn,lambda x,y:addFeature(x,[int(y.rstrip().split()[1]),y.rstrip().split()[0]]),list())\n",
    "def getVectors(fn):\n",
    "    return processLines(fn,lambda x,y:addFeature(x,y.rstrip().split('\\t')[1].split(',')),list())\n",
    "def getIntVectors(fn):\n",
    "    return processLines(fn,lambda x,y:addFeature(x,map(int,y.rstrip().split('\\t')[1].split(','))),list())\n",
    "class MentionData:\n",
    "    def __init__(self,x_file,y_file,feature_dict,label_dict):\n",
    "        self.feature2id = dict(getAllFeatures(feature_dict))\n",
    "        self.id2feature = dict(getAllFeaturesRev(feature_dict))\n",
    "        self.id2label = dict(getAllFeaturesRev(label_dict))\n",
    "        self.label2id = dict(getAllFeatures(label_dict))\n",
    "        \n",
    "        self.data = []\n",
    "        for x,y in self.readData(x_file,y_file):\n",
    "            labels = np.zeros(len(self.id2label),dtype=float)\n",
    "            labels[y]=1.\n",
    "            self.data.append(Instance([f for f in x if f in self.id2feature] ,labels,y))\n",
    "    def readData(self,x_file,y_file):\n",
    "        \n",
    "        assert count_lines(x_file) == count_lines(y_file)\n",
    "        return zip(getIntVectors(x_file),getIntVectors(y_file))\n",
    "class Instance:\n",
    "    def __init__(self,features,labels,sparse_labels):\n",
    "        self.features = np.asarray(features,dtype=int)\n",
    "        self.labels = labels\n",
    "        self.sparse_labels = sparse_labels\n",
    "        self.negative_labels = self.get_negatives()\n",
    "    def get_negatives(self):\n",
    "        return [i for i in range(len(self.labels)) if self.labels[i] <1.]\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_dir= \"/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN\"\n",
    "a=MentionData('/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/train_x_new.txt',\n",
    "              \"/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/train_y.txt\",\n",
    "             in_dir+\"/feature.txt\",in_dir+\"/type.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(A,B,insts,size,lr,max_it =10):\n",
    "    for it in xrange(1,max_it+1):\n",
    "        error = 0.\n",
    "\n",
    "        for i,inst in enumerate(insts):\n",
    "            error+=cgradient(A,B,inst,size,lr=lr)\n",
    "        \n",
    "            if i % 1000 ==0:\n",
    "                sys.stdout.write(\"\\rIteration %d \" % (it)+ \"trained {0:.0f}%\".format(float(i)*100/len(insts))+\" Loss:{0:.2f}\".format(error))\n",
    "                sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\n\")\n",
    "def gradient(A,B,inst,size,lr=0.01):\n",
    "    #TODO\n",
    "    dA = np.zeros(size)\n",
    "    dB = np.zeros([len(inst.labels),size])\n",
    "    x = np.sum(A[inst.features],axis=0)\n",
    "    error = 0.\n",
    "    neg_num = len(inst.negative_labels)\n",
    "    for l in inst.sparse_labels:\n",
    "        s1= np.vdot(x,B[l])\n",
    "        N=1\n",
    "        n_sample  = -1\n",
    "        for k in xrange(neg_num):\n",
    "            nl = inst.negative_labels[randint(0,neg_num-1)]\n",
    "            s2 = np.vdot(x,B[nl])\n",
    "            if s1 - s2<1:\n",
    "                n_sample = nl\n",
    "                N = k+1\n",
    "                break\n",
    "        if n_sample!=-1:\n",
    "            L = crank(len(inst.negative_labels)/N)\n",
    "            error += (1+s2-s1)*L\n",
    "            dA += L*(B[l]-B[n_sample])\n",
    "            dB[l] += L*x\n",
    "            dB[nl] -= L*x\n",
    "    for f in inst.features:\n",
    "        A[f] += lr*dA\n",
    "        norm = np.linalg.norm(A[f])\n",
    "        if norm >1:\n",
    "            A[f] /= norm\n",
    "    for i in xrange(len(B)):\n",
    "        B[i] += lr*dB[i]\n",
    "        norm =  np.linalg.norm(B[i])\n",
    "        if norm >1:\n",
    "            B[i] /=norm\n",
    "    return error\n",
    "def rank(k):\n",
    "    loss = 0.\n",
    "    for i in xrange(1,k+1):\n",
    "        loss += 1./i\n",
    "    return loss\n",
    "def save_to_text(matrix,output):\n",
    "    shape = matrix.shape\n",
    "    with open(output,'wb') as out:\n",
    "        out.write(\"%d %d\\n\" % (shape))\n",
    "        for row in matrix:\n",
    "            x = \" \".join(map(lambda x:\"{0:.5}\".format(x),row))\n",
    "            out.write(x+\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size=50\n",
    "A= np.random.rand(len(a.feature2id),size)\n",
    "B= np.random.rand(len(a.label2id),size)\n",
    "train(A,B,a.data,50,lr=0.01,max_it=10)\n",
    "save_to_text(A,'/Users/mayk/working/figer/baseline/PLE/Results/warp_py_A.txt')\n",
    "save_to_text(B,'/Users/mayk/working/figer/baseline/PLE/Results/warp_py_B.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "\n",
    "cimport numpy as np\n",
    "from random import randint\n",
    "\n",
    "cdef crank(int k):\n",
    "    cdef float loss = 0.\n",
    "    cdef int i = 1\n",
    "    for i in xrange(1,k+1):\n",
    "        loss += 1./i\n",
    "    return loss\n",
    "def cgradient(A,B,inst,size,lr=0.01):\n",
    "    dA = np.zeros(size)\n",
    "    dB = np.zeros([len(inst.labels),size])\n",
    "    x = np.sum(A[inst.features],axis=0)\n",
    "    error = 0.\n",
    "    neg_num = len(inst.negative_labels)\n",
    "    cdef int i =0\n",
    "    for l in inst.sparse_labels:\n",
    "        s1= np.vdot(x,B[l])\n",
    "        N=1\n",
    "        n_sample  = -1\n",
    "        for k in xrange(neg_num):\n",
    "            nl = inst.negative_labels[randint(0,neg_num-1)]\n",
    "            s2 = np.vdot(x,B[nl])\n",
    "            if s1 - s2<1:\n",
    "                n_sample = nl\n",
    "                N = k+1\n",
    "                break\n",
    "        if n_sample!=-1:\n",
    "            L = crank(len(inst.negative_labels)/N)\n",
    "            error += (1+s2-s1)*L\n",
    "            dA += L*(B[l]-B[n_sample])\n",
    "            dB[l] += L*x\n",
    "            dB[nl] -= L*x\n",
    "    for f in inst.features:\n",
    "        A[f] += lr*dA\n",
    "        norm = np.linalg.norm(A[f])\n",
    "        if norm >1:\n",
    "            A[f] /= norm\n",
    "    for i in xrange(len(B)):\n",
    "        B[i] += lr*dB[i]\n",
    "        norm =  np.linalg.norm(B[i])\n",
    "        if norm >1:\n",
    "            B[i] /=norm\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit crank(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit rank(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit gradient(A,B,a.data[2],50,lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit cgradient(A,B,a.data[2],50,lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit np.vdot(B[1],B[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "from libc.math cimport exp\n",
    "from libc.math cimport log\n",
    "from libc.string cimport memset\n",
    "\n",
    "# scipy <= 0.15\n",
    "\n",
    "import scipy.linalg.blas as fblas\n",
    "\n",
    "REAL = np.float32\n",
    "cdef extern from \"/Users/mayk/working/figer/baseline/PLE/Model/warp/voidptr.h\":\n",
    "    void* PyCObject_AsVoidPtr(object obj)\n",
    "DEF MAX_SENTENCE_LEN = 10000\n",
    "cdef scopy_ptr scopy = <scopy_ptr>PyCObject_AsVoidPtr(fblas.scopy._cpointer)  # y = x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
