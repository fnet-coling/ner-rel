{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "from sampler import *\n",
    "from eval import eval_triples\n",
    "from model import *\n",
    "from model.comp_models import *\n",
    "import sys\n",
    "from kb import subsample_kb\n",
    "import shutil\n",
    "import json\n",
    "from tensorflow.models.rnn.rnn_cell import *\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops.seq2seq import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('in_dir', \"/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN\", 'data dir containing extracted files of fb15k dataset.')\n",
    "\n",
    "# model\n",
    "tf.app.flags.DEFINE_integer(\"size\", 50, \"hidden size of model\")\n",
    "\n",
    "# training\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 1e-2, \"Learning rate.\")\n",
    "tf.app.flags.DEFINE_float(\"l2_lambda\", 0, \"L2-regularization raten (only for batch training).\")\n",
    "tf.app.flags.DEFINE_float(\"sample_text_prob\", 0.935,\n",
    "                          \"Probability of sampling text triple (default is ratio of text (emnlp) to kb triples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lazy_readlines(fn):\n",
    "    with open(fn,'r') as f:\n",
    "        for ln in f:\n",
    "            yield ln\n",
    "def processLines(fn,reduce_fn,init):\n",
    "    return reduce(reduce_fn,lazy_readlines(fn),init)\n",
    "def count_lines(fn):\n",
    "    return processLines(fn,lambda x,y:x+1,0)\n",
    "def addFeature(x,y):\n",
    "    x.append(y)\n",
    "    return x\n",
    "def getAllFeatures(fn):\n",
    "    return processLines(fn,lambda x,y:addFeature(x,[y.rstrip().split()[0],int(y.rstrip().split()[1])]),list())\n",
    "def getAllFeaturesRev(fn):\n",
    "    return processLines(fn,lambda x,y:addFeature(x,[int(y.rstrip().split()[1]),y.rstrip().split()[0]]),list())\n",
    "def getVectors(fn):\n",
    "    return processLines(fn,lambda x,y:addFeature(x,y.rstrip().split('\\t')[1].split(',')),list())\n",
    "def getIntVectors(fn):\n",
    "    return processLines(fn,lambda x,y:addFeature(x,map(int,y.rstrip().split('\\t')[1].split(','))),list())\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = dict(getAllFeaturesRev(FLAGS.in_dir+\"/feature.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def max_margin_loss(prediction):\n",
    "    # TODO implement max_margin loss\n",
    "    return None\n",
    "\n",
    "class NERModel():\n",
    "    def __init__(self,features,labels,size,num_pos,num_neg,lr=1e-2):\n",
    "        self.features = features\n",
    "        self._init = tf.random_normal_initializer(0.0,0.1)\n",
    "        self.size = size\n",
    "        self.labels = labels\n",
    "        with vs.variable_scope(self.name(),initializer=self._init):\n",
    "            self.learning_rate =tf.Variable(float(lr),trainable=False,name='lr')\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                self.opt= tf.train.AdamOptimizer(self.learning_rate,beta1=0.0)\n",
    "            self._init_input()\n",
    "\n",
    "            with vs.variable_scope(\"score\",initializer=self._init):\n",
    "                self._scores = self.score()\n",
    "        scores = tf.reshape(self._scores,[num_pos,num_neg+1])\n",
    "        labels = np.zeros([num_pos,num_neg+1],dtype=np.float32)\n",
    "        labels[:,0]= 1\n",
    "        labels = tf.constant(labels,name = 'labels_constant',dtype=tf.float32)\n",
    "        loss = math_ops.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(scores,labels))\n",
    "        self._loss = loss / math_ops.cast(num_pos,dtypes.float32)\n",
    "        self.saver = tf.train.Saver(tf.all_variables())\n",
    "        \n",
    "    def name(self):\n",
    "        return self.__class__.__name__\n",
    "    def _init_input(self):\n",
    "        self.feature_input = tf.placeholder(tf.int64,shape=[None],name ='X')\n",
    "        self.label_input = tf.placeholder(tf.int64,shape=[None],name ='Y')\n",
    "\n",
    "    def score(self):\n",
    "        with tf.device('/cpu:0'):\n",
    "            E_f = tf.get_variable('E_f',[len(self.features),self.size])\n",
    "            E_l = tf.get_variable('E_l',[len(self.labels),self.size])\n",
    "        self.e_feature = tf.nn.embedding_lookup(E_f,self.feature_input)\n",
    "        self.e_l = tf.nn.embedding_lookup(E_l,self.label_input)\n",
    "        score = tf_util.batch_dot(self.e_feature,self.e_l)\n",
    "        return score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = NERModel(features,a.label2id.keys(),FLAGS.size,100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MentionData:\n",
    "    def __init__(self,x_file,y_file,feature_dict,label_dict):\n",
    "        self.feature2id = dict(getAllFeatures(feature_dict))\n",
    "        self.id2feature = dict(getAllFeaturesRev(feature_dict))\n",
    "        self.id2label = dict(getAllFeaturesRev(label_dict))\n",
    "        self.label2id = dict(getAllFeatures(label_dict))\n",
    "        \n",
    "        self.data = []\n",
    "        for x,y in self.readData(x_file,y_file):\n",
    "            labels = np.zeros(len(self.id2label),dtype=float)\n",
    "            labels[y]=1.\n",
    "            self.data.append(Instance([f for f in x if f in self.id2feature] ,labels,y))\n",
    "    def readData(self,x_file,y_file):\n",
    "        \n",
    "        assert count_lines(x_file) == count_lines(y_file)\n",
    "        return zip(getIntVectors(x_file),getIntVectors(y_file))\n",
    "class Instance:\n",
    "    def __init__(self,features,labels,sparse_labels):\n",
    "        self.features = np.asarray(features,dtype=int)\n",
    "        self.labels = labels\n",
    "        self.sparse_labels = sparse_labels\n",
    "        self.negative_labels = self.get_negatives()\n",
    "    def get_negatives(self):\n",
    "        return [i for i in range(len(self.labels)) if self.labels[i] <1.]\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=MentionData('/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/train_x_new.txt',\n",
    "              \"/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/train_y.txt\",\n",
    "             FLAGS.in_dir+\"/feature.txt\",FLAGS.in_dir+\"/type.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from random import randint\n",
    "def train(A,B,insts,size,lr,max_it =10):\n",
    "    for it in xrange(1,max_it+1):\n",
    "        error = 0.\n",
    "\n",
    "        for i,inst in enumerate(insts):\n",
    "            error+=gradient(A,B,inst,size,lr=lr)\n",
    "        \n",
    "            if i % 1000 ==0:\n",
    "                sys.stdout.write(\"\\rIteration %d \" % (it)+ \"trained {0:.0f}%\".format(float(i)*100/len(insts))+\" Loss:{0:.2f}\".format(error))\n",
    "                sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\n\")\n",
    "def gradient(A,B,inst,size,lr=0.01):\n",
    "    #TODO\n",
    "    dA = np.zeros(size)\n",
    "    dB = np.zeros([len(inst.labels),size])\n",
    "    x = np.sum(A[inst.features],axis=0)\n",
    "    error = 0.\n",
    "    neg_num = len(inst.negative_labels)\n",
    "    for l in inst.sparse_labels:\n",
    "        s1= np.vdot(x,B[l])\n",
    "        N=1\n",
    "        n_sample  = -1\n",
    "        for k in xrange(neg_num):\n",
    "            nl = inst.negative_labels[randint(0,neg_num-1)]\n",
    "            s2 = np.vdot(x,B[nl])\n",
    "            if s1 - s2<1:\n",
    "                n_sample = nl\n",
    "                N = k+1\n",
    "                break\n",
    "        if n_sample!=-1:\n",
    "            L = rank(len(inst.negative_labels)/N)\n",
    "            error += (1+s2-s1)*L\n",
    "            dA += L*(B[l]-B[n_sample])\n",
    "            dB[l] += L*x\n",
    "            dB[nl] -= L*x\n",
    "    for f in inst.features:\n",
    "        A[f] += lr*dA\n",
    "        norm = np.linalg.norm(A[f])\n",
    "        if norm >1:\n",
    "            A[f] /= norm\n",
    "    for i in xrange(len(B)):\n",
    "        B[i] += lr*dB[i]\n",
    "        norm =  np.linalg.norm(B[i])\n",
    "        if norm >1:\n",
    "            B[i] /=norm\n",
    "    return error\n",
    "def rank(k):\n",
    "    loss = 0.\n",
    "    for i in xrange(1,k+1):\n",
    "        loss += 1./i\n",
    "    return loss\n",
    "def save_to_text(matrix,output):\n",
    "    shape = matrix.shape\n",
    "    with open(output,'wb') as out:\n",
    "        out.write(\"%d %d\\n\" % (shape))\n",
    "        for row in matrix:\n",
    "            x = \" \".join(map(lambda x:\"{0:.5}\".format(x),row))\n",
    "            out.write(x+\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 trained 100% Loss:149849.73\n",
      "Iteration 2 trained 100% Loss:81449.28\n",
      "Iteration 3 trained 100% Loss:61424.35\n",
      "Iteration 4 trained 100% Loss:48747.49\n",
      "Iteration 5 trained 100% Loss:42367.50\n",
      "Iteration 6 trained 100% Loss:37299.68\n",
      "Iteration 7 trained 100% Loss:32811.31\n",
      "Iteration 8 trained 100% Loss:30475.46\n",
      "Iteration 9 trained 100% Loss:27403.82\n",
      "Iteration 10 trained 1% Loss:277.77"
     ]
    }
   ],
   "source": [
    "A= np.random.rand(len(a.feature2id),FLAGS.size)/5000000\n",
    "B= np.random.rand(len(a.label2id),FLAGS.size)/5000000\n",
    "train(A,B,a.data,50,lr=0.01,max_it=10)\n",
    "save_to_text(A,'/Users/mayk/working/figer/baseline/PLE/Results/warp_py_A.txt')\n",
    "save_to_text(B,'/Users/mayk/working/figer/baseline/PLE/Results/warp_py_B.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.choice(inst.negative_labels,len(inst.negative_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27406001,  0.36153903,  0.21621207, ...,  0.5250929 ,\n",
       "         0.33020648,  0.99836781],\n",
       "       [ 0.98064453,  0.18360787,  0.36516393, ...,  0.67615391,\n",
       "         0.59949142,  0.71523378],\n",
       "       [ 0.2827323 ,  0.41887949,  0.8727263 , ...,  0.59032071,\n",
       "         0.82324863,  0.38877511],\n",
       "       ..., \n",
       "       [ 0.21594779,  0.8739337 ,  0.01000087, ...,  0.11775628,\n",
       "         0.6064113 ,  0.03424501],\n",
       "       [ 0.31961715,  0.39138731,  0.00469416, ...,  0.87721133,\n",
       "         0.44588859,  0.06237578],\n",
       "       [ 0.27849131,  0.66276299,  0.19716125, ...,  0.75256482,\n",
       "         0.66414742,  0.93027942]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(len(a.feature2id),FLAGS.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44, 45,  0,  9, 13, 28, 29, 22, 16, 33, 13, 25, 11,  2, 27, 19, 27,\n",
       "       22,  9, 27,  9, 23, 12, 40,  8, 44, 21, 15, 35, 13,  0, 33, 14,  1,\n",
       "       33, 18, 41, 29, 11, 25, 14, 45, 37])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst = a.data[1]\n",
    "np.random.choice(inst.negative_labels,len(inst.negative_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inst.negative_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
